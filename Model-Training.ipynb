{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "functions-test-run.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetprakhar/TALKFiles/blob/main/Model-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evsOKo9kjUnY",
        "outputId": "89caa2c0-dca6-496e-9898-778549f60b10"
      },
      "source": [
        "!unzip Python.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  Python.zip\n",
            "   creating: Python/\n",
            "  inflating: Python/sample_angle_data.csv  \n",
            "  inflating: Python/plot_angle_data.py  \n",
            "  inflating: Python/requirements.txt  \n",
            "  inflating: Python/filter_data.py   \n",
            "   creating: Python/data/\n",
            "  inflating: Python/data/DataCircle1.csv  \n",
            "  inflating: Python/data/DataSquare1_new.csv  \n",
            "  inflating: Python/functions-test-run.ipynb  \n",
            "  inflating: Python/utils.py         \n",
            "   creating: Python/__pycache__/\n",
            "  inflating: Python/__pycache__/plot_angle_data.cpython-37.pyc  \n",
            "  inflating: Python/__pycache__/filter_data.cpython-37.pyc  \n",
            "   creating: Python/local/\n",
            "  inflating: Python/local/model.py   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-gCXVVO7E0I",
        "outputId": "c08c2af9-8c22-4724-c54a-1502777abc28"
      },
      "source": [
        "%cd Python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-O80XEUiidm",
        "outputId": "c220f5dc-6a1f-4de9-e7d0-392d14996610"
      },
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchsummary import summary\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "from scipy.signal import savgol_filter as sgolayfilt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "\n",
        "def calc_delta(arr):\n",
        "    del_arr = np.zeros_like(arr)\n",
        "    del_arr[1:] = np.array(\n",
        "        list([arr[i] - arr[i-1] for i in range(1, len(arr))]))\n",
        "    return del_arr\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "# Deterministic Behaviour\n",
        "seed_everything(420)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNKwpMpZiid5"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, lr, epochs, optimizer, criterion, confidence=0.4, metric_name='accuracy'):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.epoch=0\n",
        "        self.loss = 0\n",
        "        self.acc = 0\n",
        "        self.name = metric_name\n",
        "        self.loss_history = {'train':[], 'val':[]}\n",
        "        self.acc_history = {'train':[], 'val':[]}\n",
        "        self.optimizer_name = optimizer\n",
        "        self.confidence = confidence\n",
        "        \n",
        "        self.criterion = criterion\n",
        "        if isinstance(optimizer, str) and optimizer.lower() == \"lbfgs\":\n",
        "            self.optimizer = torch.optim.LBFGS(self.model.parameters(), lr=lr)\n",
        "        else:\n",
        "            self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
        "      \n",
        "    def closure(self, X, y):        \n",
        "        def _closure():\n",
        "            self.optimizer.zero_grad()  # Clear existing gradient\n",
        "\n",
        "            output = self.model(X)\n",
        "            # loss = self.criterion(output.contiguous().view(output.shape[1], -1), y.contiguous().view(y.shape[1], -1))\n",
        "            loss = self.criterion(output[0], y[0])\n",
        "            # print(output.shape, y.shape, loss.shape)\n",
        "            \n",
        "            # loss = loss[(y.sum(2) > 0).squeeze()]  # Ignore loss from zeros\n",
        "            \n",
        "            loss = loss.mean()\n",
        "            loss.backward()\n",
        "\n",
        "            self.loss = loss.item()\n",
        "            self.loss_history['train'].append(self.loss)\n",
        "            self.output = output\n",
        "            self.acc = self.accuracy(self.output, y)\n",
        "            self.acc_history['train'].append(self.acc)\n",
        "\n",
        "            print('\\rEpoch: {}/{}.............'.format(self.epoch, self.epochs), end=' ')\n",
        "            print(\"Loss: {:.4f} {name}: {:.4f}\".format(\n",
        "                self.loss, self.acc, name=self.name), end='')\n",
        "            return loss\n",
        "        return _closure\n",
        "\n",
        "    def start(self, X_train, y_train, X_val, y_val):\n",
        "        for epoch in range(self.epochs):\n",
        "            self.epoch = epoch\n",
        "            for X, y in zip(X_train, y_train):\n",
        "                closure = self.closure(X[None, ...], y)\n",
        "                \n",
        "                # Slightly different syntax for LBFGS\n",
        "                if isinstance(self.optimizer_name, str) and self.optimizer_name.lower() == \"lbfgs\":\n",
        "                    self.optimizer.step(closure)\n",
        "                else:\n",
        "                    _ = closure().item()\n",
        "                    self.optimizer.step()  # Update the weights\n",
        "\n",
        "            # Validation\n",
        "            with torch.no_grad():\n",
        "                losses = []\n",
        "                accs = []\n",
        "                \n",
        "                # Run for each example\n",
        "                for X, y in zip(X_val, y_val):\n",
        "                    output_val = self.model(X[None, ...])\n",
        "                    # outputs.append(output_val)\n",
        "                    # loss = self.criterion(output_val.contiguous().view(output_val.shape[1], -1), y.contiguous().view(y.shape[1], -1))\n",
        "                    loss = self.criterion(output_val[0], y[0])\n",
        "                    loss = loss[(y.sum(2) > 0).squeeze()]\n",
        "                    loss = loss.mean()\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                    acc_val = self.accuracy(output_val, y)\n",
        "                    accs.append(acc_val)\n",
        "\n",
        "                self.val_loss = sum(losses) / len(losses)\n",
        "                self.val_acc = sum(accs) / len(accs)\n",
        "                self.loss_history['val'].append(self.val_loss)\n",
        "                self.acc_history['val'].append(self.val_acc)\n",
        "                \n",
        "\n",
        "            print('\\rEpoch: {}/{}.............'.format(self.epoch, self.epochs), end=' ')\n",
        "            print(\"Avg. Loss: {:.4f} Avg. {name}: {:.4f}, Avg. Val_Loss: {:.4f} Avg. {name}: {:.4f}\".format(\n",
        "                self.loss, self.acc, self.val_loss, self.val_acc, name=self.name), end='\\n')\n",
        "    def mapping(self, y):\n",
        "        y_ = y > self.confidence + 0.0\n",
        "        if y_.sum() == 0:\n",
        "            return 0\n",
        "        elif y_[0] == 1:\n",
        "            return 1 if y[0] > y[1] else 2\n",
        "        elif y_[1] == 1:\n",
        "            return 2 if y[1] > y[0] else 1\n",
        "\n",
        "    def accuracy(self, y_pred, y_true):\n",
        "        # print(y_pred, y_pred.max(1),  y_pred.min(1))\n",
        "        y_pred = np.array(list(map(self.mapping, (y_pred).detach().cpu().numpy().squeeze())))\n",
        "        y_true = np.array(list(map(self.mapping, y_true.detach().cpu().numpy().squeeze())))\n",
        "        # acc = (np.array(list(map(mapping, y_pred.squeeze()))) == np.array(list(map(mapping, y_true.squeeze())))).sum() / len(y_true.squeeze())\n",
        "        acc = ((y_pred == y_true) * (y_true != 0)).sum() / sum((y_true != 0))\n",
        "        return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2m9HA-qiieE",
        "outputId": "251e7d06-1bb6-4c6c-ff22-15913cc1c839"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_cells, n_fc=None, fc_act=nn.ReLU(), out_act=nn.Sigmoid()):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_cells, batch_first=True)\n",
        "        self.n_fc = n_fc\n",
        "        \n",
        "        last_dim = hidden_dim\n",
        "        if self.n_fc is not None:\n",
        "            self.fc = []\n",
        "            for n_dims in n_fc:\n",
        "                self.fc.append(nn.Linear(last_dim, n_dims))\n",
        "                last_dim = n_dims\n",
        "            self.fc = nn.ModuleList(self.fc)\n",
        "                \n",
        "        self.fc_act = fc_act  # For use in the top dense layers\n",
        "        self.out_fc = nn.Linear(last_dim, output_size)\n",
        "        self.act = out_act  # For use at the output layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "#         print(x.shape)  #DEBUG\n",
        "        out, hidden = self.lstm(x)\n",
        "#         print(out.shape)  #DEBUG\n",
        "        if self.n_fc is not None:\n",
        "            for fc in self.fc:\n",
        "                out = fc(out)\n",
        "                out = self.fc_act(out)\n",
        "#                 print(out.shape)  #DEBUG\n",
        "        \n",
        "        out = self.out_fc(out)\n",
        "        out = self.act(out)\n",
        "        # print(out.shape)  #DEBUG\n",
        "        return out\n",
        "    \n",
        "# Sanity test\n",
        "data = np.ones((1, 1300, 3))\n",
        "data = torch.from_numpy(data.astype(np.float32)).cuda()\n",
        "\n",
        "model = LSTM(\n",
        "    input_size=3,\n",
        "    output_size=1,\n",
        "    hidden_dim=10,\n",
        "    n_cells=2,\n",
        "    n_fc=[5, 3],\n",
        "    fc_act=nn.Tanh(),\n",
        ").cuda()\n",
        "# model = LSTM(3, 1, 10, 2, [5, 3]).cuda()\n",
        "model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6372],\n",
              "         [0.6376],\n",
              "         [0.6375],\n",
              "         ...,\n",
              "         [0.6371],\n",
              "         [0.6371],\n",
              "         [0.6371]]], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggo8SgEDiieM"
      },
      "source": [
        "def load_data(folder, delta=False, svgolay_filt=False, window_length=11, polyorder=3, normalize='minmax', one_hot=False, split=0.8):\n",
        "    csvs = glob.glob(os.path.join(folder, '*.csv'))\n",
        "    \n",
        "    x_train = []\n",
        "    x_val = []\n",
        "    y_train = []\n",
        "    y_val = []\n",
        "    \n",
        "    # Read the CSVs\n",
        "    for csv in csvs:\n",
        "        d = pd.read_csv(csv)\n",
        "        l = d.iloc[:, 3]\n",
        "        d = d.iloc[:, :3]\n",
        "        \n",
        "        # Apply delta and/or svgolay filtering\n",
        "        if delta or svgolay_filt:\n",
        "            deltax = calc_delta(d.iloc[:, 0])\n",
        "            deltay = calc_delta(d.iloc[:, 1])\n",
        "            deltaz = calc_delta(d.iloc[:, 2])\n",
        "            \n",
        "            if svgolay_filt:\n",
        "                deltax = pd.Series(sgolayfilt(deltax, window_length, polyorder))\n",
        "                deltay = pd.Series(sgolayfilt(deltay, window_length, polyorder))\n",
        "                deltaz = pd.Series(sgolayfilt(deltaz, window_length, polyorder))\n",
        "            else:\n",
        "                deltax = pd.Series(deltax)\n",
        "                deltay = pd.Series(deltay)\n",
        "                deltaz = pd.Series(deltaz)\n",
        "            \n",
        "            d = pd.concat([deltax, deltay, deltaz], axis=1)\n",
        "        \n",
        "        # Normalize\n",
        "        if normalize is not None:\n",
        "            assert normalize in ['minmax', 'mean'], \"Must be one of ['minmax', 'mean']\"\n",
        "            d = (d - d.mean(axis=0)) / (d.max(axis=0) - d.min(axis=0)) if 'minmax' else \\\n",
        "                (d - d.mean(axis=0)) / d.std(axis=0)\n",
        "        \n",
        "        # Convert to float32 numpy array to comply with torch cuda backend\n",
        "        x_train.append(torch.from_numpy(d.iloc[:int(split * len(d)), :].values.astype(np.float32)).cuda())\n",
        "        x_val.append(torch.from_numpy(d.iloc[int(split * len(d)):, :].values.astype(np.float32)).cuda())\n",
        "        y_train.append(l[:int(split * len(d))].values[..., None])\n",
        "        y_val.append(l[int(split * len(d)):].values[..., None])\n",
        "        \n",
        "    # Convert to one-hot encoding and drop 0th column, belonging to background\n",
        "    if one_hot:\n",
        "        enc = ohe()\n",
        "        enc.fit(np.vstack(y_train + y_val))\n",
        "        y_train = [enc.transform(j).toarray()[:, 1:].astype(np.float32) for j in y_train]\n",
        "        y_val = [enc.transform(j).toarray()[:, 1:].astype(np.float32) for j in y_val]\n",
        "\n",
        "    y_train = list(map(lambda x: torch.from_numpy(x).float()[None, ...].to('cuda'), y_train))\n",
        "    y_val = list(map(lambda x: torch.from_numpy(x).float()[None, ...].to('cuda'), y_val))\n",
        "    return x_train, y_train, x_val, y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChfX7Y0SiieR",
        "outputId": "7e865951-81bf-4e15-85bb-0ae520de3b38"
      },
      "source": [
        "X_train, y_train, X_val, y_val = load_data(\"./data\", one_hot=True, delta=True, svgolay_filt=True, normalize=None, split=0.7)\n",
        "# X_train, y_train, X_val, y_val = [torch.cat(X_train)], [torch.cat(y_train, dim=1)], [torch.cat(X_val)], [torch.cat(y_val, dim=1)]\n",
        "X_train, y_train, X_val, y_val = torch.cat(X_train), torch.cat(y_train, dim=1), torch.cat(X_val), torch.cat(y_val, dim=1)\n",
        "\n",
        "_, y_train_, _, y_val_ = load_data(\"./data\", one_hot=False, delta=True, svgolay_filt=True, normalize=None, split=0.7)\n",
        "y_train_, y_val_ = torch.cat(y_train_, dim=1).ravel(), torch.cat(y_val_, dim=1).ravel()\n",
        "\n",
        "# Reduce zeros\n",
        "y_val_p = (y_val_.cpu().numpy() == 0) / (y_val_.cpu().numpy() == 0).sum()\n",
        "y_train_p = (y_train_.cpu().numpy() == 0) / (y_train_.cpu().numpy() == 0).sum()\n",
        "y_val_idx = np.random.choice(np.arange(len(y_val_)), size=1250, replace=False, p=y_val_p)\n",
        "y_train_idx = np.random.choice(np.arange(len(y_train_)), size=3000, replace=False, p=y_train_p)\n",
        "\n",
        "y_val = pd.DataFrame(y_val.cpu().numpy().squeeze())\n",
        "y_train = pd.DataFrame(y_train.cpu().numpy().squeeze())\n",
        "X_val = pd.DataFrame(X_val.cpu().numpy())\n",
        "X_train = pd.DataFrame(X_train.cpu().numpy())\n",
        "\n",
        "y_val.drop(y_val_idx, inplace=True)\n",
        "y_train.drop(y_train_idx, inplace=True)\n",
        "X_val.drop(y_val_idx, inplace=True)\n",
        "X_train.drop(y_train_idx, inplace=True)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.values).cuda()\n",
        "X_val = torch.from_numpy(X_val.values).cuda()\n",
        "y_train = torch.from_numpy(y_train.values).cuda()\n",
        "y_val = torch.from_numpy(y_val.values).cuda()\n",
        "\n",
        "model = LSTM(\n",
        "    input_size=3,\n",
        "    output_size=2,\n",
        "    hidden_dim=10,\n",
        "    n_cells=1,\n",
        "    n_fc=[8],\n",
        "    fc_act=nn.ReLU(),\n",
        "    # fc_act=nn.Tanh(),\n",
        "    out_act=nn.Sigmoid()\n",
        ").cuda()\n",
        "\n",
        "(model(X_train[None, ...].cuda()) - y_train[0]).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5471, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "6sLFHLkGiInj",
        "outputId": "82fd73df-3aa3-4338-e5c4-33ae4612dc4c"
      },
      "source": [
        "def mapping(y):\n",
        "    y_ = y > 0.4 + 0.0\n",
        "    if y_.sum() == 0:\n",
        "        return 0\n",
        "    elif y_[0] == 1:\n",
        "        return 1 if y[0] > y[1] else 2\n",
        "    elif y_[1] == 1:\n",
        "        return 2 if y[1] > y[0] else 1\n",
        "\n",
        "plt.hist(np.array(list(map(mapping, y_train))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2254.,    0.,    0.,    0.,    0., 1340.,    0.,    0.,    0.,\n",
              "        1877.]),\n",
              " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSYT1Dj-iieT",
        "outputId": "57bc0ef1-207e-4593-9b38-999d64328b91"
      },
      "source": [
        "trainer = Trainer(model, lr=2e-3, epochs=5, optimizer='lbfgs', criterion=nn.BCELoss(reduce=False, reduction='none'), confidence=0.04)\n",
        "l = trainer.start([X_train], [y_train[None, ...]], [X_val], [y_val[None, ...]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/5............. Avg. Loss: 0.7468 Avg. accuracy: 0.8928, Avg. Val_Loss: 0.5044 Avg. accuracy: 0.9055\n",
            "Epoch: 1/5............. Avg. Loss: 0.7333 Avg. accuracy: 0.8949, Avg. Val_Loss: 0.5020 Avg. accuracy: 0.9074\n",
            "Epoch: 2/5............. Avg. Loss: 0.5575 Avg. accuracy: 0.9397, Avg. Val_Loss: 0.4867 Avg. accuracy: 0.9145\n",
            "Epoch: 3/5............. Avg. Loss: 0.5245 Avg. accuracy: 0.9394, Avg. Val_Loss: 0.4840 Avg. accuracy: 0.9119\n",
            "Epoch: 4/5............. Avg. Loss: 0.4598 Avg. accuracy: 0.9381, Avg. Val_Loss: 0.4981 Avg. accuracy: 0.8997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4yZQnvIGBP-"
      },
      "source": [
        "import pickle\n",
        "i = 3.1\n",
        "assert not os.path.exists(f'model{i}.pth'), \"Choose unique name\"\n",
        "\n",
        "trainer1 = vars(trainer)\n",
        "torch.save(model.state_dict(), f'model{i}.pth')\n",
        "pickle.dump(trainer1, open(f'trainer{i}.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcMD5X80iiea"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA-xwAqg-2n1",
        "outputId": "6dbda203-8e83-4852-aa71-546d4b5efe90"
      },
      "source": [
        "!zip models *.pth\n",
        "!zip trainers *.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: model1.pth (deflated 30%)\n",
            "  adding: model2.pth (deflated 30%)\n",
            "  adding: model3.1.pth (deflated 30%)\n",
            "  adding: model3.pth (deflated 30%)\n",
            "  adding: trainer1.pkl (deflated 29%)\n",
            "  adding: trainer2.pkl (deflated 29%)\n",
            "  adding: trainer3.1.pkl (deflated 30%)\n",
            "  adding: trainer3.pkl (deflated 29%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0QZRRklsPil"
      },
      "source": [
        " for f in glob.glob('trainer*.pkl'):\n",
        "    t = pickle.load(open(f, 'rb'))\n",
        "    t['model'] = None\n",
        "    t['optimizer'] = None\n",
        "    t['criterion'] = None\n",
        "    t['output'] = None\n",
        "    pickle.dump(t, open(f'_{f}', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "237i73D0teht",
        "outputId": "0cb277cb-4ddf-4407-9a06-f97f8d471651"
      },
      "source": [
        "!zip _trainers _*.pkl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: _trainer1.pkl (deflated 63%)\n",
            "  adding: _trainer2.pkl (deflated 63%)\n",
            "  adding: _trainer3.1.pkl (deflated 52%)\n",
            "  adding: _trainer3.pkl (deflated 57%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRhBxyIvuAcJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}